{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "from collections import deque\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import highway_env\n",
    "#highway_env.register_highway_envs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"parking-v0\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('observation',\n",
       "               array([0.        , 0.        , 0.        , 0.        , 0.91130559,\n",
       "                      0.41173064])),\n",
       "              ('achieved_goal',\n",
       "               array([0.        , 0.        , 0.        , 0.        , 0.91130559,\n",
       "                      0.41173064])),\n",
       "              ('desired_goal',\n",
       "               array([ 2.200000e-01, -1.400000e-01,  0.000000e+00,  0.000000e+00,\n",
       "                       6.123234e-17, -1.000000e+00]))]),\n",
       " {'speed': 0,\n",
       "  'crashed': False,\n",
       "  'action': array([-0.68331754, -0.16529469], dtype=float32),\n",
       "  'is_success': False})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"KinematicsGoal\",\n",
    "        \"features\": ['x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'],\n",
    "        \"scales\": [100, 100, 5, 5, 1, 1],\n",
    "        \"normalize\": False\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"ContinuousAction\"\n",
    "    },\n",
    "    \"simulation_frequency\": 15,\n",
    "    \"policy_frequency\": 5,\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 300,\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 7,\n",
    "    \"show_trajectories\": False,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False,\n",
    "}\n",
    "\n",
    "env.unwrapped.configure(config)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_actions = []\n",
    "\n",
    "for steering in np.linspace(-0.5, 0.5, 3):\n",
    "    for acceleration in np.linspace(0.8, 0.4, 3):\n",
    "        candidate_actions.append(torch.Tensor([acceleration, steering]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__ (self, state_size, hidden_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_size, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, action_size)\n",
    "\n",
    "        # Called with either one element to determine next action, or a batch\n",
    "        # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayMemory (object) : \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not collections.OrderedDict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Sélection et exécution d'une action\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_done\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     next_state, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Stockage de la transition dans la mémoire\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 28\u001b[0m, in \u001b[0;36mselect_action\u001b[1;34m(state, steps_done)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Convertir state en tensor PyTorch si nécessaire\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# print(\"state\", state)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample \u001b[38;5;241m>\u001b[39m eps_threshold:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;66;03m# Utiliser le réseau pour choisir une action\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not collections.OrderedDict"
     ]
    }
   ],
   "source": [
    "# Hyperparamètres\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "LEARNING_RATE = 0.001\n",
    "num_episodes = 50\n",
    "\n",
    "policy_net = DQN(8*8*7, 128, 9)\n",
    "target_net = DQN(8*8*7, 128, 9)\n",
    "\n",
    "\n",
    "# Initialisation de l'optimiseur\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def select_action(state, steps_done):\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    \n",
    "    # Convertir state en tensor PyTorch si nécessaire\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "        # print(\"state\", state)\n",
    "        state = torch.tensor([state], dtype=torch.float) \n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # Utiliser le réseau pour choisir une action\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        # Choisir une action au hasard\n",
    "        return torch.tensor([[random.randrange(6)]], dtype=torch.long)\n",
    "\n",
    "# Fonction d'optimisation\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Ici, vous préparerez les données et exécuterez une étape d'optimisation.\n",
    "\n",
    "steps_done = 0\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialisation de l'environnement et de l'état\n",
    "    state, _ = env.reset()\n",
    "    # env.render()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Sélection et exécution d'une action\n",
    "        action = select_action(state, steps_done)\n",
    "        next_state, reward, done, truncated, _ = env.step(action.item())\n",
    "\n",
    "        # Stockage de la transition dans la mémoire\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Déplacement vers le nouvel état\n",
    "        state = next_state\n",
    "\n",
    "        # Performer une étape d'optimisation sur le batch actuel\n",
    "        optimize_model()\n",
    "\n",
    "    # Mise à jour du réseau cible, copiant tous les poids du réseau principal\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Entraînement complet')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
